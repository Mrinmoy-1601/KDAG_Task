{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13334c09",
   "metadata": {},
   "source": [
    "# Subtask 1 : Mathematical Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8390108c",
   "metadata": {},
   "source": [
    "## Definition of Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8f0411",
   "metadata": {},
   "source": [
    "Naive Bayes predicts the value using the bayes theorem and assuming that each and every observation is independent of each other. It chooses the value with the greatest probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d9bbf",
   "metadata": {},
   "source": [
    "## Underlying Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534dfc80",
   "metadata": {},
   "source": [
    "The Naive Bayes algorithm calculates the probability of each case using Bayes Theorem and predicts the value according to the probabaility. The class with the highest probability is assigned to the x value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e5cc0b",
   "metadata": {},
   "source": [
    "## Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471fb68e",
   "metadata": {},
   "source": [
    "The assumption we take during the creation of our model is that each and every observation are independent of each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee23a40c",
   "metadata": {},
   "source": [
    "## Equations Involved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3095ff3d",
   "metadata": {},
   "source": [
    "### Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a69ba7",
   "metadata": {},
   "source": [
    "P(y|X) = P(X|y) * P(y) / P(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4cb817",
   "metadata": {},
   "source": [
    "posterior probability = P(y|X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b76b1",
   "metadata": {},
   "source": [
    "prior probability = P(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5a659b",
   "metadata": {},
   "source": [
    "likelihood probability (conditional probability) = P(X|y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7275021",
   "metadata": {},
   "source": [
    "where y is the class variable and X represents the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b30a2f",
   "metadata": {},
   "source": [
    "as X = [x1, x2, x3, x4, x5, ......]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad471e08",
   "metadata": {},
   "source": [
    "P(y|X) = P(y|x1) * P(y|x2) * .... * P(y|xn) * P(y) / P(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa7ae2",
   "metadata": {},
   "source": [
    "For every value in the dataset the P(X) does not change so it can be removed. Also as probabilities can also have very low values, we take log on both sides to iognore the zero overflow error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a5f403",
   "metadata": {},
   "source": [
    "log (P(y|X)) = log(P(y|x1)) + log(P(y|x2)) + .... + log(P(y|xn)) + log(P(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58233df4",
   "metadata": {},
   "source": [
    "As we have more than one case of y, we need to find the class y with maximum probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e2f1a",
   "metadata": {},
   "source": [
    "So, y would be the class whose posterior probability will be maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad3b91",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57560783",
   "metadata": {},
   "source": [
    "The conditional probability P(X|y) changes to: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a140a8",
   "metadata": {},
   "source": [
    "P(X|y) = exp(-((x-mean)^2)/(2 * var))/(sqrt(2 * pi * var))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e4eae",
   "metadata": {},
   "source": [
    "where exp means exponentian, x represents each feature, var represent the variance of that feature, mean represents the mean of each feature, X represents all the features and y represents the class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e9f7ab",
   "metadata": {},
   "source": [
    "## Working of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb2cf2a",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f29b4",
   "metadata": {},
   "source": [
    "The model calculates the mean, variance of all the features and classes and also the prior probability of all the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faee4eaa",
   "metadata": {},
   "source": [
    " ### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc921407",
   "metadata": {},
   "source": [
    "It uses the Bayes Theorem to calculate the posterior probabaility and then it returns the class with the highest posterior probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c1de03",
   "metadata": {},
   "source": [
    "# Subtask 2 : Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84860ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf50222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining my model of Naive_Bayes\n",
    "class Naive_Bayes():\n",
    "    \n",
    "    def __init__(self, alpha = 0.1):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        self.mean = np.zeros((n_classes,n_features))\n",
    "        self.var = np.zeros((n_classes,n_features))\n",
    "        self.prior = np.zeros(n_classes)\n",
    "        \n",
    "        for idx,c in enumerate(self.classes):\n",
    "            X_c = X[y==c]\n",
    "            self.mean[idx,:] = X_c.mean(axis=0)\n",
    "            self.var[idx,:] = X_c.var(axis=0)\n",
    "            self.prior[idx] = X_c.shape[0]/n_samples\n",
    "    \n",
    "    def predict(self,X):\n",
    "        y_pred = [self.pred(x) for x in X]\n",
    "        return y_pred\n",
    "    \n",
    "    def pred(self,x):\n",
    "        posteriors = []\n",
    "        for idx,c in enumerate(self.classes):\n",
    "            prior = np.log(self.prior[idx])\n",
    "            likelihood = np.sum(np.log(self.cond_prob(idx,x)))\n",
    "            posterior = prior + likelihood\n",
    "            posteriors.append(posterior)\n",
    "        prediction = self.classes[np.argmax(posteriors)]\n",
    "        return prediction\n",
    "    \n",
    "    def cond_prob(self,idx,x):\n",
    "        mean = self.mean[idx]\n",
    "        var = self.var[idx] + self.alpha\n",
    "        num = np.exp((-((x-mean)**2))/(2*var))\n",
    "        den = np.sqrt(2*np.pi*var)\n",
    "        P_x_y = num/den\n",
    "        return P_x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27465347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the files\n",
    "train1 = pd.read_csv('ds1_train.csv')\n",
    "test1 = pd.read_csv('ds1_test.csv')\n",
    "train2 = pd.read_csv('ds2_train.csv')\n",
    "test2 = pd.read_csv('ds2_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ecd7ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating the x and y values\n",
    "X_train1 = train1.iloc[:,:-1].values\n",
    "y_train1 = train1.iloc[:,-1].values\n",
    "X_test1 = test1.iloc[:,:-1].values\n",
    "y_test1 = test1.iloc[:,-1].values\n",
    "X_train2 = train2.iloc[:,:-1].values\n",
    "y_train2 = train2.iloc[:,-1].values\n",
    "X_test2 = test2.iloc[:,:-1].values\n",
    "y_test2 = test2.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0dc82e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the accuracy function\n",
    "def accuracy(y_pred,y_test):\n",
    "    acc = (np.sum(y_pred==y_test))/len(y_test)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d45e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to get the accuracy for the dataset\n",
    "def printing_accuracy(X_train, y_train, X_test, y_test):\n",
    "    nb = Naive_Bayes()\n",
    "    nb.fit(X_train,y_train)\n",
    "    y_pred = nb.predict(X_test)\n",
    "    acc = accuracy(y_pred,y_test)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e8b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the accuracies of the predictions for the datasets\n",
    "my_acc_train_1 = printing_accuracy(X_train1, y_train1, X_train1, y_train1)\n",
    "my_acc_test_1 = printing_accuracy(X_train1, y_train1, X_test1, y_test1)\n",
    "my_acc_train_2 = printing_accuracy(X_train2, y_train2, X_train2, y_train2)\n",
    "my_acc_test_2 = printing_accuracy(X_train2, y_train2, X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e963269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the ds1_test.csv 0.83\n",
      "accuracy of the ds1_train.csv 0.82375\n",
      "accuracy of the ds2_test.csv 0.92\n",
      "accuracy of the ds2_train.csv 0.91625\n"
     ]
    }
   ],
   "source": [
    "#Printing the accuracies of the predictions done by the model\n",
    "print('accuracy of the ds1_test.csv',my_acc_test_1)\n",
    "print('accuracy of the ds1_train.csv',my_acc_train_1)\n",
    "print('accuracy of the ds2_test.csv',my_acc_test_2)\n",
    "print('accuracy of the ds2_train.csv',my_acc_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1939e6",
   "metadata": {},
   "source": [
    "# Subtask 3 : Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a077f343",
   "metadata": {},
   "source": [
    "The hyperparameter for the model is alpha also known as the Laplace smoothing parameter which is used to handle the zero probability cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4374c0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to get the best alpha parameter\n",
    "def best(X_train,y_train,X_test,y_test):\n",
    "    best_alpha = 0\n",
    "    best_acc = 0\n",
    "    alpha = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "    for i in alpha:\n",
    "        nb = Naive_Bayes(alpha = i)\n",
    "        nb.fit(X_train, y_train)\n",
    "        y_pred = nb.predict(X_test)\n",
    "        acc = accuracy(y_pred,y_test)\n",
    "        \n",
    "        if acc>best_acc:\n",
    "            best_alpha = i\n",
    "            best_acc = acc\n",
    "    print('Best value of alpha for the given dataset : ',best_alpha)\n",
    "    print('Best accuracy for the best alpha for the given dataset : ',best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3abc214c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset - 1\n",
      "Best value of alpha for the given dataset :  0.3\n",
      "Best accuracy for the best alpha for the given dataset :  0.84\n"
     ]
    }
   ],
   "source": [
    "# printing the best alpha for dataset 1\n",
    "print('Dataset - 1')\n",
    "best(X_train1, y_train1, X_test1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc5530a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset - 2\n",
      "Best value of alpha for the given dataset :  0\n",
      "Best accuracy for the best alpha for the given dataset :  0.92\n"
     ]
    }
   ],
   "source": [
    "# printing the best alpha for dataset 2\n",
    "print('Dataset - 2')\n",
    "best(X_train2, y_train2, X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbdd997",
   "metadata": {},
   "source": [
    "# Subtask 4 : Comparison with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de195826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3501a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining scikit learn naive bayes model\n",
    "def scikit_naive_bayes(X_train, y_train, X_test, y_test):\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    y_pred = nb.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test,y_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56da6aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the test dataset 1 :  0.83\n",
      "Accuracy for the train dataset 1 :  0.8225\n",
      "Accuracy for the test dataset 2 :  0.92\n",
      "Accuracy for the train dataset 2 :  0.915\n"
     ]
    }
   ],
   "source": [
    "# printing the accuracies of the scikit learn model\n",
    "acc_test_1 = scikit_naive_bayes(X_train1,y_train1,X_test1,y_test1)\n",
    "print('Accuracy for the test dataset 1 : ',acc_test_1)\n",
    "acc_train_1 = scikit_naive_bayes(X_train1,y_train1,X_train1,y_train1)\n",
    "print('Accuracy for the train dataset 1 : ',acc_train_1)\n",
    "acc_test_2 = scikit_naive_bayes(X_train2,y_train2,X_test2,y_test2)\n",
    "print('Accuracy for the test dataset 2 : ',acc_test_2)\n",
    "acc_train_2 = scikit_naive_bayes(X_train2,y_train2,X_train2,y_train2)\n",
    "print('Accuracy for the train dataset 2 : ',acc_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d6ddab",
   "metadata": {},
   "source": [
    "## Comparison between two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51a38fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-Learn Logistic Regression Model :: ds1_test.csv ::  0.83\n",
      "My Model :: ds1_test.csv ::  0.83\n",
      "Scikit-Learn Logistic Regression Model :: ds1_train.csv ::  0.8225\n",
      "My Model :: ds1_test.csv ::  0.82375\n",
      "Scikit-Learn Logistic Regression Model :: ds2_test.csv ::  0.92\n",
      "My Model :: ds2_test.csv ::  0.92\n",
      "Scikit-Learn Logistic Regression Model :: ds2_train.csv ::  0.915\n",
      "My Model :: ds2_test.csv ::  0.91625\n"
     ]
    }
   ],
   "source": [
    "# printing the comparison between the two models\n",
    "print('Scikit-Learn Logistic Regression Model :: ds1_test.csv :: ',acc_test_1)\n",
    "print('My Model :: ds1_test.csv :: ',my_acc_test_1)\n",
    "print('Scikit-Learn Logistic Regression Model :: ds1_train.csv :: ',acc_train_1)\n",
    "print('My Model :: ds1_test.csv :: ',my_acc_train_1)\n",
    "print('Scikit-Learn Logistic Regression Model :: ds2_test.csv :: ',acc_test_2)\n",
    "print('My Model :: ds2_test.csv :: ',my_acc_test_2)\n",
    "print('Scikit-Learn Logistic Regression Model :: ds2_train.csv :: ',acc_train_2)\n",
    "print('My Model :: ds2_test.csv :: ',my_acc_train_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
