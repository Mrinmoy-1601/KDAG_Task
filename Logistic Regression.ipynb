{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "513ea7af",
   "metadata": {},
   "source": [
    "# Subtask 1 : Mathematical Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d101f5c",
   "metadata": {},
   "source": [
    "## Definition of Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e9686",
   "metadata": {},
   "source": [
    "Logistic Regression Analysis is a predictive modelling technique. It estimates a relation between a dependent variable and an independent variable. Logistic Regression is built on linear regression but it is modified to work on a particular case. It produces results in a binary format (either 0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c3adb3",
   "metadata": {},
   "source": [
    "## Underlying principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b57c701",
   "metadata": {},
   "source": [
    "Logistic regression assumes a linear relationship between the features and the sigmoid(S Curve) of the target variable. It uses the sigmoid function to predict the binary value(0 or 1) of the dependent variable according to the given independent variable representing the positive class. The threshold value is set to be 0.5. Any value above it is the positive class (binary value set to 1) and below it is the negative class (binary value set to 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a6f92",
   "metadata": {},
   "source": [
    "## Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c96229",
   "metadata": {},
   "source": [
    "The first assumption taken is that the independent variable should be binary. The second assumption is that there should be linear relation between the dependent variable and sigmoid of the independent variable. The model also assumes that the observations should be independent of each other. Also there should not be any multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5b525c",
   "metadata": {},
   "source": [
    "## Equations Involved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d00f88",
   "metadata": {},
   "source": [
    "### Linear Regression Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad115ece",
   "metadata": {},
   "source": [
    "f(x) = c + b1x1 + b2x2 + ...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d071ce29",
   "metadata": {},
   "source": [
    "### Logistic Regression Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf4a0bf",
   "metadata": {},
   "source": [
    "log(y/1-y) = c + b1x1 + b2x2 + ....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0dbd6",
   "metadata": {},
   "source": [
    "### Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcfbba2",
   "metadata": {},
   "source": [
    "y = 1/(1 + e^(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cc6829",
   "metadata": {},
   "source": [
    "### Calculating Error Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd6d13c",
   "metadata": {},
   "source": [
    "J(w,b) = 1/N (Σ(i=1 to N)[(yi * log(f(xi))) + ((1-yi) * log(1-f(xi)))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db55721c",
   "metadata": {},
   "source": [
    "J'(w) = 1/N (Σ(i=1 to N)[2 * xi * (y^ - yi)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b019559",
   "metadata": {},
   "source": [
    "J'(b) = 1/N (Σ(i=1 to N)[2 * (y^ - yi)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403f358a",
   "metadata": {},
   "source": [
    "where N is the number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3952648",
   "metadata": {},
   "source": [
    "## Working of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bf3e7d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71345ddc",
   "metadata": {},
   "source": [
    "The model parameters weight and bias are learned by minimizing the J(w,b) using the optimization algorithm,i.e., gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e3dbea",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f4f9f",
   "metadata": {},
   "source": [
    "The probabaility is calculated using the sigmoid function. If the probability exceeds the threshold (which is 0.5), it is classified as the positive class(value =1) otherwise, it is classified as the negative class(value = 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b87241f",
   "metadata": {},
   "source": [
    "# Subtask 2 : Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb7a7ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c998019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python function to define the sigmoid function\n",
    "def sigmoid(x): \n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19250ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the logistic regression algorithm\n",
    "class Logistic_Regression(): \n",
    "\n",
    "    def __init__(self, lr = 0.01, n_iterations = 1000):\n",
    "        self.lr = lr\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self,X,y): \n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        for i in range(self.n_iterations): #to run for the n number of iterations\n",
    "            sigmoid_x = np.dot(X,self.weights) + self.bias\n",
    "            prediction = sigmoid(sigmoid_x) #calculating the sigmoid function\n",
    "            # calculating the gradient descent\n",
    "            dw = (1/n_samples)*(np.dot(X.T,(prediction-y)))\n",
    "            db = (1/n_samples)*(np.sum(prediction - y))\n",
    "            #updating the weights and bias of the model\n",
    "            self.weights = self.weights - self.lr*dw\n",
    "            self.bias = self.bias - self.lr*db\n",
    "    \n",
    "    def predict(self,X): #function to predict y for the given x\n",
    "        sigmoid_x = np.dot(X,self.weights) + self.bias\n",
    "        prediction = sigmoid(sigmoid_x)\n",
    "        y_pred = [0 if y<=0.5 else 1 for y in prediction]\n",
    "        return y_pred\n",
    "    def accuracy(self,y_pred,y):\n",
    "        acc = (np.sum(y_pred==y))/len(y)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c3655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the files\n",
    "train1 = pd.read_csv('ds1_train.csv')\n",
    "test1 = pd.read_csv('ds1_test.csv')\n",
    "train2 = pd.read_csv('ds2_train.csv')\n",
    "test2 = pd.read_csv('ds2_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb5e51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating the x and y values\n",
    "X_train1 = train1.iloc[:,:-1].values\n",
    "y_train1 = train1.iloc[:,-1].values\n",
    "X_test1 = test1.iloc[:,:-1].values\n",
    "y_test1 = test1.iloc[:,-1].values\n",
    "X_train2 = train2.iloc[:,:-1].values\n",
    "y_train2 = train2.iloc[:,-1].values\n",
    "X_test2 = test2.iloc[:,:-1].values\n",
    "y_test2 = test2.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73080a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the class of logistic regression\n",
    "log_reg = Logistic_Regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "290304f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model for dataset 1\n",
    "log_reg.fit(X_train1,y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e8a87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the values of dataset 1\n",
    "y_test1_pred = log_reg.predict(X_test1)\n",
    "y_train1_pred = log_reg.predict(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e54f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model for dataset 2\n",
    "log_reg.fit(X_train2,y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e9e6a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the values for dataset 2\n",
    "y_test2_pred = log_reg.predict(X_test2)\n",
    "y_train2_pred = log_reg.predict(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0647ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the ds1_test.csv 0.78\n",
      "accuracy of the ds1_train.csv 0.785\n",
      "accuracy of the ds2_test.csv 0.91\n",
      "accuracy of the ds2_train.csv 0.90875\n"
     ]
    }
   ],
   "source": [
    "#Printing the accuracies of the predictions done by the model\n",
    "print('accuracy of the ds1_test.csv',log_reg.accuracy(y_test1_pred,y_test1))\n",
    "print('accuracy of the ds1_train.csv',log_reg.accuracy(y_train1_pred,y_train1))\n",
    "print('accuracy of the ds2_test.csv',log_reg.accuracy(y_test2_pred,y_test2))\n",
    "print('accuracy of the ds2_train.csv',log_reg.accuracy(y_train2_pred,y_train2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd997f1f",
   "metadata": {},
   "source": [
    "# Subtask 3 : Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825b4fc0",
   "metadata": {},
   "source": [
    "The two important hyperparameters are learning rate and number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2fed435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a functrion to calculate the best learning rate\n",
    "def best_lr(X_train, y_train, X_test, y_test):\n",
    "    best_lr = 0\n",
    "    best_acc = 0\n",
    "    lr = [0.1,0.01,0.001]\n",
    "    for i in lr:\n",
    "        log_reg = Logistic_Regression(lr = i)\n",
    "        log_reg.fit(X_train,y_train)\n",
    "        y_pred = log_reg.predict(X_test)\n",
    "        accuracy = log_reg.accuracy(y_pred,y_test)\n",
    "    \n",
    "        if accuracy>best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_lr = i\n",
    "\n",
    "    print('Best Learning Rate for Dataset : ',best_lr)\n",
    "    print('Best Accuracy for Dataset corresponding to the best learning rate : ',best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "753dcd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1\n",
      "Best Learning Rate for Dataset :  0.01\n",
      "Best Accuracy for Dataset corresponding to the best learning rate :  0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MRINMOY DEY\\AppData\\Local\\Temp\\ipykernel_36968\\1388416440.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  y = 1/(1+np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "#Getting the best learning rate for dataset 1\n",
    "print('Dataset 1')\n",
    "best_lr(X_train1,y_train1,X_test1,y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "253e8582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 2\n",
      "Best Learning Rate for Dataset :  0.1\n",
      "Best Accuracy for Dataset corresponding to the best learning rate :  0.92\n"
     ]
    }
   ],
   "source": [
    "#Getting the best learning rate for dataset 2\n",
    "print('Dataset 2')\n",
    "best_lr(X_train2,y_train2,X_test2,y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e6d53d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a functrion to calculate the best number of iterations\n",
    "def best_n_iterations(X_train,y_train,X_test,y_test):\n",
    "    best_n_iterations = 0\n",
    "    best_acc = 0\n",
    "    n_iterations = [500,1000,5000,10000]\n",
    "    for n in n_iterations:\n",
    "        log_reg = Logistic_Regression(n_iterations = n)\n",
    "        log_reg.fit(X_train,y_train)\n",
    "        y_pred = log_reg.predict(X_test)\n",
    "        accuracy = log_reg.accuracy(y_pred,y_test)\n",
    "    \n",
    "        if accuracy>best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_n_iterations = n\n",
    "\n",
    "    print('Best No of Iterations for Dataset : ',best_n_iterations)\n",
    "    print('Best Accuracy for Dataset corresponding to the best No of iterations : ',best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c01f024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset - 1\n",
      "Best No of Iterations for Dataset :  10000\n",
      "Best Accuracy for Dataset corresponding to the best No of iterations :  0.83\n"
     ]
    }
   ],
   "source": [
    "#Getting the best number of iterations for dataset 1\n",
    "print('Dataset - 1')\n",
    "best_n_iterations(X_train1,y_train1,X_test1,y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c37180bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset - 2\n",
      "Best No of Iterations for Dataset :  5000\n",
      "Best Accuracy for Dataset corresponding to the best No of iterations :  0.92\n"
     ]
    }
   ],
   "source": [
    "#Getting the best number of iterations for dataset 2\n",
    "print('Dataset - 2')\n",
    "best_n_iterations(X_train2,y_train2,X_test2,y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde8e8ee",
   "metadata": {},
   "source": [
    "# Subtask 4 : Comparison with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a237cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the scikit-learn library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74e15ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defing a the logistic regression function using the scikit learn library\n",
    "def scikit_logistic_regression(X_train,y_train,X_test,y_test):\n",
    "    sk_log_reg = LogisticRegression()\n",
    "    sk_log_reg.fit(X_train,y_train)\n",
    "    y_pred = sk_log_reg.predict(X_test)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5817db0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the test dataset 1 :  0.9\n",
      "Accuracy for the train dataset 1 :  0.8825\n",
      "Accuracy for the test dataset 2 :  0.9\n",
      "Accuracy for the train dataset 2 :  0.915\n"
     ]
    }
   ],
   "source": [
    "#printing the accuracies for the datasets\n",
    "acc_test_1 = scikit_logistic_regression(X_train1,y_train1,X_test1,y_test1)\n",
    "print('Accuracy for the test dataset 1 : ',acc_test_1)\n",
    "acc_train_1 = scikit_logistic_regression(X_train1,y_train1,X_train1,y_train1)\n",
    "print('Accuracy for the train dataset 1 : ',acc_train_1)\n",
    "acc_test_2 = scikit_logistic_regression(X_train2,y_train2,X_test2,y_test2)\n",
    "print('Accuracy for the test dataset 2 : ',acc_test_2)\n",
    "acc_train_2 = scikit_logistic_regression(X_train2,y_train2,X_train2,y_train2)\n",
    "print('Accuracy for the train dataset 2 : ',acc_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c265274",
   "metadata": {},
   "source": [
    "## Comparison between the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "111929d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = Logistic_Regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0318f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-Learn Logistic Regression Model :: ds1_test.csv ::  0.9\n",
      "My Model :: ds1_test.csv ::  0.78\n",
      "Scikit-Learn Logistic Regression Model :: ds1_train.csv ::  0.8825\n",
      "My Model :: ds1_test.csv ::  0.785\n",
      "Scikit-Learn Logistic Regression Model :: ds2_test.csv ::  0.9\n",
      "My Model :: ds2_test.csv ::  0.91\n",
      "Scikit-Learn Logistic Regression Model :: ds2_train.csv ::  0.915\n",
      "My Model :: ds2_test.csv ::  0.90875\n"
     ]
    }
   ],
   "source": [
    "print('Scikit-Learn Logistic Regression Model :: ds1_test.csv :: ',acc_test_1)\n",
    "print('My Model :: ds1_test.csv :: ',log_reg.accuracy(y_test1_pred,y_test1))\n",
    "print('Scikit-Learn Logistic Regression Model :: ds1_train.csv :: ',acc_train_1)\n",
    "print('My Model :: ds1_test.csv :: ',log_reg.accuracy(y_train1_pred,y_train1))\n",
    "print('Scikit-Learn Logistic Regression Model :: ds2_test.csv :: ',acc_test_2)\n",
    "print('My Model :: ds2_test.csv :: ',log_reg.accuracy(y_test2_pred,y_test2))\n",
    "print('Scikit-Learn Logistic Regression Model :: ds2_train.csv :: ',acc_train_2)\n",
    "print('My Model :: ds2_test.csv :: ',log_reg.accuracy(y_train2_pred,y_train2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
